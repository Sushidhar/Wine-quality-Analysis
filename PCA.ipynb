{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_b3df59f4cfa4437ba5a0d8341462d910(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', 'aef925cbc55c424b9be33df912da34c9')\n",
    "    hconf.set(prefix + '.username', '482344af2f77465fb158947814a1d548')\n",
    "    hconf.set(prefix + '.password', 'ExnZeWW7-2,.,S#t')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', False)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_b3df59f4cfa4437ba5a0d8341462d910(name)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_data_1 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferschema', 'true')\\\n",
    "  .load('swift://DefaultProjectsushidharjayaramanmavsutaedu.' + name + '/white.csv')\n",
    "df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages = [] # stages in our Pipeline\n",
    "  \n",
    "label_stringIdx = StringIndexer(inputCol = \"quality\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "#numericCols = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "numericCols = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"total sulfur dioxide\",\"free sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "#numericCols = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"chlorides\",\"free sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "#numericCols = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"chlorides\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "#assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "cols = df_data_1.columns\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(df_data_1)\n",
    "dataset = pipelineModel.transform(df_data_1)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "#display(dataset)\n",
    "#type(dataset)\n",
    "dataset.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(dataset)\n",
    "\n",
    "result = model.transform(dataset).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}